{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most recent version of this notebook is available at https://github.com/nadiinchi/dl_labs/blob/master/lab_attention.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.bmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def get_scores(self, features, queries):\n",
    "        \"\"\"\n",
    "        features: [batch_size x num_objects x obj_feature_dim]\n",
    "        queries:  [batch_size x num_queries x query_feature_dim]\n",
    "        Returns matrix of scores with shape [batch_size x num_queries x num_objects].\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()                \n",
    "\n",
    "    def attend(self, features, queries, mask=None):\n",
    "        \"\"\"\n",
    "        features:        [batch_size x num_objects x obj_feature_dim]\n",
    "        queries:         [batch_size x num_queries x query_feature_dim]\n",
    "        mask, optional:  [batch_size x num_queries x num_objects]\n",
    "        Returns matrix of features for queries with shape [batch_size x num_queries x obj_feature_dim].\n",
    "        If mask is not None, set corresponding to mask weights to zero.\n",
    "        Saves detached weights as self.last_weights for further visualization.\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        w = self.get_scores(queries, features);\n",
    "        if mask:\n",
    "            w *= (1 - mask)\n",
    "        w = torch.softmax(w, dim=1)\n",
    "        a = torch.bmm(w, features)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AdditiveAttention(Attention):\n",
    "    \"\"\"\n",
    "    Bahdanau et al. \"Neural Machine Translation by Jointly Learning to Align and Translate\", 2014.\n",
    "    \"\"\"\n",
    "    def __init__(self, obj_feature_dim, query_feature_dim, hidden_dim):\n",
    "        \"\"\"\n",
    "        obj_feature_dim   - dimensionality of attention object features vector\n",
    "        query_feature_dim - dimensionality of attention query vector\n",
    "        hidden_dim        - dimensionality of latent vectors of attention \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # your code here\n",
    "        self.w3 = torch.randn(hidden_dim)\n",
    "        self.w1 = torch.randn((hidden_dim, query_feature_dim))\n",
    "        self.w2 = torch.randn((hidden_dim, obj_feature_dim))\n",
    "        \n",
    "\n",
    "    def get_scores(self, features, queries):\n",
    "        \"\"\"\n",
    "        features: [batch_size x num_objects x obj_feature_dim]\n",
    "        queries:  [batch_size x num_queries x query_feature_dim]\n",
    "        Returns matrix of scores with shape [batch_size x num_queries x num_objects].\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        x1 = torch.matmul(self.w1.transpose(), queries)\n",
    "        x2 = torch.matmul(self.w2.transpose(), features)\n",
    "        result = torch.bmm(self.w3, torch.tanh(x1 + x2))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MultiplicativeAttention(Attention):\n",
    "    \"\"\"\n",
    "    Luong et al. \"Effective approaches to attention-based neural machine translation\", 2015.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def get_scores(self, features, queries):\n",
    "        \"\"\"\n",
    "        features: [batch_size x num_objects x feature_dim]\n",
    "        queries:  [batch_size x num_queries x feature_dim]\n",
    "        Returns matrix of scores with shape [batch_size x num_queries x num_objects].\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        result = torch.bmm(queries, features)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(Attention):\n",
    "    \"\"\"\n",
    "    Vaswani et al. \"Attention Is All You Need\", 2017.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def get_scores(self, features, queries):\n",
    "        \"\"\"\n",
    "        features: [batch_size x num_objects x feature_dim]\n",
    "        queries:  [batch_size x num_queries x feature_dim]\n",
    "        Returns matrix of scores with shape [batch_size x num_queries x num_objects].\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        result = torch.bmm(features, queries) / queries.shape[1]\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "transpose() missing 2 required positional argument: \"dim0\", \"dim1\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-bdfe959abf41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mqueries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_queries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_feature_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mattention1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdditiveAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_feature_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mres1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-b8b35377115f>\u001b[0m in \u001b[0;36mattend\u001b[0;34m(self, features, queries, mask)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \"\"\"\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# your code here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mw\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-4f7fdbd52447>\u001b[0m in \u001b[0;36mget_scores\u001b[0;34m(self, features, queries)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \"\"\"\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# your code here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: transpose() missing 2 required positional argument: \"dim0\", \"dim1\""
     ]
    }
   ],
   "source": [
    "# time to check that your attention works\n",
    "# your code here\n",
    "batch_size = 10\n",
    "num_objects = 1\n",
    "num_queries = 1\n",
    "feature_dim = 1\n",
    "hidden_dim = 1\n",
    "query_feature_dim = 1\n",
    "features = torch.zeros((batch_size, num_objects, feature_dim))\n",
    "queries = torch.zeros((batch_size, num_queries, query_feature_dim))\n",
    "attention1 = AdditiveAttention(feature_dim, query_feature_dim, hidden_dim)\n",
    "res1 = attention1.attend(features, queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "attention2 = MultiplicativeAttention()\n",
    "res2 = attention2.attend(features, queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "attention3 = ScaledDotProductAttention()\n",
    "res3 = attention3.attend(features, queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perm_generator(batch_size, perm_size):\n",
    "    \"\"\"\n",
    "    Generates batch of batch_size objects.\n",
    "    Each object consists of two random permutations with length perm_size.\n",
    "    The target for the object is the product of its two permutations.\n",
    "    \"\"\"\n",
    "    # your code here\n",
    "    return objects, correct_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# time to check your generator\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, dim, max_len=50, scale=10000.0, add=True):\n",
    "        \"\"\"\n",
    "        Transforms input as described by Vaswani et al. in \"Attention Is All You Need\", 2017.\n",
    "        dim     - dimension of positional embeddings.\n",
    "        max_len - maximal length of sequence, for precomputing\n",
    "        scale   - scale factor for frequency for positional embeddings\n",
    "        add     - boolean, if add is False, concatenate positional embeddings with input instead of adding\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dim = dim\n",
    "        self.add = add\n",
    "        if add:\n",
    "            self.extra_output_shape = 0\n",
    "        else:\n",
    "            self.extra_output_shape = dim\n",
    "\n",
    "        # your code here\n",
    "               \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        input - [batch_size x sequence_len x features_dim]\n",
    "        If self.add is True, self.dim = featurs_dim.\n",
    "        Returns input with added or concatenated positional embeddings (depending on self.add).\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# time to draw positional encoder\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_autoregressive_mask(size):\n",
    "    \"\"\"\n",
    "    Returns attention mask of given size for autoregressive model.\n",
    "    \"\"\"\n",
    "    # your code here\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PermMultiplier(nn.Module):\n",
    "    def __init__(self, perm_len, embedding_dim, hidden_dim, attention, pos_enc, autoregressive):\n",
    "        \"\"\"\n",
    "        perm_len       - permutation length (the input is twice longer)\n",
    "        embedding_dim  - dimensionality of integer embeddings\n",
    "        hidden_dim     - dimensionality of LSTM output\n",
    "        attention      - Attention object\n",
    "        pos_enc        - PositionalEncoder object or None\n",
    "        autoregressive - boolean, if True, then model must use autoregressive mask for attention\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.autoregressive = autoregressive\n",
    "        self.perm_len = perm_len\n",
    "        # your code here\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Perform forward pass through layers:\n",
    "        + get embeddings from input sequence (using both embeddings\n",
    "          and positional embeddings if pos_enc is not None)\n",
    "        + run LSTM on embeddings\n",
    "        + use output of LSTM as an attention queries\n",
    "        + attend on the embedded sequence using queries (note autoregressive flag)\n",
    "        + make final linear tranformation to obtain logits\n",
    "        \"\"\"\n",
    "        # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "perm_len = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# time to set up a model\n",
    "# you can check that without pos_enc model doesn't work\n",
    "# not-autoregressive model can be learned easily, but it is less isefull\n",
    "# try to learn autoregressive model if possible\n",
    "pos_enc = PositionalEncoder(?, perm_len * 2, ?, ?)\n",
    "attention = ?\n",
    "model = PermMultiplier(perm_len, ?, ?, attention, pos_enc, ?)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up optimizer\n",
    "gd = optim.Adam(model.parameters(), lr=?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# do optimization\n",
    "avg_loss = None\n",
    "forget = 0.99\n",
    "batch_size = 64\n",
    "iterator = range(?)\n",
    "for i in iterator:\n",
    "    gd.zero_grad()\n",
    "    batch = perm_generator(batch_size, perm_len)\n",
    "    if torch.cuda.is_available():\n",
    "        batch = batch[0].cuda(), batch[1].cuda()\n",
    "    # compute batch loss\n",
    "    # your code here\n",
    "    loss.backward()\n",
    "    if avg_loss is None:\n",
    "        avg_loss = float(loss)\n",
    "    else:\n",
    "        avg_loss = forget * avg_loss + (1 - forget) * float(loss)\n",
    "    descr_str = 'Iteration %05d, loss %.5f.' % (i, avg_loss)\n",
    "    print('\\r', descr_str, end='')\n",
    "    gd.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# time to check your model\n",
    "batch = perm_generator(batch_size, perm_len)\n",
    "if torch.cuda.is_available():\n",
    "    batch = batch[0].cuda(), batch[1].cuda()\n",
    "print('Input:\\n', batch[0][:5])\n",
    "print('Output:\\n', ?)\n",
    "print('Correct:\\n', batch[1][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# visualize attention map for some object\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# play with model and learn something new about attention!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
