{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Глубинное обучение. Семинар и домашнее задание 1. Обучение полносвязной нейронной сети на numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании вы обучите полносвязную нейронную сеть распознавать рукописные цифры (а что же еще, если не их :), [почти] самостоятельно реализовав все составляющие алгоритма обучения и предсказания.\n",
    "\n",
    "[__ Конспект с выводом формул__](https://github.com/nadiinchi/dl_labs/blob/master/nn_gradients.pdf)\n",
    "\n",
    "Для начала нам понадобится реализовать прямой и обратный проход через слои. Наши слои будут соответствовать следующему интерфейсу (на примере \"тождественного\" слоя):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class IdentityLayer:\n",
    "    \"\"\"\n",
    "    A building block. Each layer is capable of performing two things:\n",
    "    \n",
    "    - Process input to get output:           \n",
    "    output = layer.forward(input)\n",
    "    \n",
    "    - Propagate gradients through itself:    \n",
    "    grad_input = layer.backward(input, grad_output)\n",
    "    \n",
    "    Some layers also have learnable parameters.\n",
    "    \n",
    "    Modified code from cs.hse DL course *\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Here you can initialize layer parameters (if any) \n",
    "        and auxiliary stuff. You should enumerate all parameters\n",
    "        in self.params\"\"\"\n",
    "        # An identity layer does nothing\n",
    "        self.params = []\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Takes input data of shape [batch, input_units], \n",
    "        returns output data [batch, output_units]\n",
    "        \"\"\"\n",
    "        # An identity layer just returns whatever it gets as input.\n",
    "        self.input = input\n",
    "        return input\n",
    "\n",
    "    def backward(self, grad_output): \n",
    "        \"\"\"\n",
    "        Performs a backpropagation step through the layer, \n",
    "        with respect to the given input.\n",
    "        \n",
    "        To compute loss gradients w.r.t input, \n",
    "        you need to apply chain rule (backprop):\n",
    "        \n",
    "        d loss / d input  = (d loss / d layer) *  (d layer / d input)\n",
    "        \n",
    "        Luckily, you already receive d loss / d layer as input, \n",
    "        so you only need to multiply it by d layer / d x.\n",
    "        \n",
    "        The method returns:\n",
    "        * gradient w.r.t input (will be passed to \n",
    "          previous layer's backward method)\n",
    "        * flattened gradient w.r.t. parameters (with .ravel() \n",
    "          applied to each gradient). \n",
    "          If there are no params, return []\n",
    "        \"\"\"\n",
    "        # The gradient of an identity layer is precisely grad_output\n",
    "        input_dim = self.input.shape[1]\n",
    "        \n",
    "        d_layer_d_input = np.eye(input_dim)\n",
    "        \n",
    "        return np.dot(grad_output, d_layer_d_input), [] # chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Реализация слоев"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала реализуем слой нелинейности $ReLU(x) = max(x, 0)$. Параметров у слоя нет. Метод forward должен вернуть результат поэлементного применения ReLU к входному массиву, метод backward - градиент функции потерь по входу слоя. В нуле будем считать производную равной 0. Обратите внимание, что при обратном проходе могут понадобиться величины, посчитанные во время прямого прохода, поэтому их стоит сохранить как атрибут класса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    \"\"\"\n",
    "    Modified code from cs.hse DL course *\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"ReLU layer simply applies elementwise rectified linear unit to all inputs\"\"\"\n",
    "        self.params = [] # ReLU has no parameters\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"Apply elementwise ReLU to [batch, num_units] matrix\"\"\"\n",
    "#         output = np.zeros(input.shape)\n",
    "        output = np.clip(input, 0, None)\n",
    "        self.values = output\n",
    "        return output\n",
    "        \n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"Compute gradient of loss w.r.t. ReLU input\n",
    "        grad_output shape: [batch, num_units]\n",
    "        output 1 shape: [batch, num_units]\n",
    "        output 2: []\n",
    "        \"\"\"\n",
    "        dx, x = None, self.values\n",
    "        dx = np.array(grad_output, copy=True)\n",
    "        dx[x <= 0] = 0\n",
    "        return dx, []\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее реализуем полносвязный слой без нелинейности. У слоя два параметра: матрица весов и вектор сдвига."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание на второй аргумент: в нем надо возвращать градиент по всем параметрам в одномерном виде. Для этого надо сначала применить .ravel() ко всем градиентам, а затем воспользоваться  np.r_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  0.,  1.,  2.,  3.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "np.r_[np.eye(3).ravel(), np.arange(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    \"\"\"\n",
    "    Modified code from cs.hse DL course *\n",
    "    \"\"\"\n",
    "    def __init__(self, input_units, output_units):\n",
    "        \"\"\"\n",
    "        A dense layer is a layer which performs a learned affine transformation:\n",
    "        f(x) = W x + b\n",
    "        \"\"\"\n",
    "        # initialize weights with small random numbers from normal distribution\n",
    "        self.weights = np.random.randn(input_units, output_units)*0.01\n",
    "        self.biases = np.zeros(output_units)\n",
    "        self.params = [self.weights, self.biases]\n",
    "        \n",
    "    def forward(self,input):\n",
    "        \"\"\"\n",
    "        Perform an affine transformation:\n",
    "        f(x) = W x + b\n",
    "        \n",
    "        input shape: [batch, input_units]\n",
    "        output shape: [batch, output units]\n",
    "        \"\"\"\n",
    "        self.x = input\n",
    "        output = np.dot(self.x, self.weights) + self.biases\n",
    "        return output\n",
    "        \n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        compute gradients\n",
    "        grad_output shape: [batch, output_units]\n",
    "        output shapes: [batch, input_units], [num_params]\n",
    "        \n",
    "        hint: use function np.r_\n",
    "        np.r_[np.arange(3), np.arange(3)] = [0, 1, 2, 0, 1, 2]\n",
    "        \"\"\"\n",
    "        dx = np.dot(grad_output, self.weights.T)\n",
    "        dw = np.dot(self.x.T, grad_output)\n",
    "        db = np.dot(grad_output.T, np.ones(grad_output.shape[0]))\n",
    "        return dx, np.r_[dw.ravel(), db]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверка градиента"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим правильность реализации с помощью функции численной проверки градиента. Функция берет на вход callable объект (функцию от одного аргумента-матрицы) и аргумент и вычисляет приближенный градиент функции в этой точке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_numerical_gradient(f, x, verbose=False, h=0.00001):\n",
    "    \"\"\"Evaluates gradient df/dx via finite differences:\n",
    "    df/dx ~ (f(x+h) - f(x-h)) / 2h\n",
    "    Adopted from https://github.com/ddtm/dl-course/\n",
    "    \"\"\"\n",
    "    fx = f(x) # evaluate function value at original point\n",
    "    grad = np.zeros_like(x)\n",
    "    # iterate over all indexes in x\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "\n",
    "        # evaluate function at x+h\n",
    "        ix = it.multi_index\n",
    "        oldval = x[ix]\n",
    "        x[ix] = oldval + h # increment by h\n",
    "        fxph = f(x) # evalute f(x + h)\n",
    "        x[ix] = oldval - h\n",
    "        fxmh = f(x) # evaluate f(x - h)\n",
    "        x[ix] = oldval # restore\n",
    "\n",
    "        # compute the partial derivative with centered formula\n",
    "        grad[ix] = (fxph - fxmh) / (2 * h) # the slope\n",
    "        if verbose:\n",
    "            print (ix, grad[ix])\n",
    "        it.iternext() # step to next dimension\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычислите аналитический и численный градиенты по входу слоя ReLU от функции\n",
    "$$ f(y) = \\sum_i y_i, \\quad y = ReLU(x) $$\n",
    "\n",
    "Следующая ячейка после заполнения должна не выдавать ошибку :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "points = np.linspace(-1, 1, 10*12).reshape([10, 12])\n",
    "relu = ReLU()\n",
    "y = relu.forward(points)\n",
    "f = lambda x: np.sum(relu.forward(x))\n",
    "grads = relu.backward(np.ones(y.shape))[0]\n",
    "numeric_grads = eval_numerical_gradient(f, points) \n",
    "\n",
    "assert np.allclose(grads, numeric_grads, rtol=1e-3, atol=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычислите аналитический и численный градиенты по входу полносвязного слоя от функции\n",
    "$$ f(y) = \\sum_i y_i, \\quad y = W x + b $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-1, 1, 10*12).reshape([10, 12])\n",
    "l = Dense(12, 32,)\n",
    "y = l.forward(x)\n",
    "f = lambda p: np.sum(l.forward(p))\n",
    "grads = l.backward(np.ones(y.shape))[0]\n",
    "numeric_grads = eval_numerical_gradient(f, x) \n",
    "\n",
    "assert np.allclose(grads, numeric_grads, rtol=1e-3, atol=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Реализация softmax-слоя и функции потерь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для решения задачи многоклассовой классификации обычно используют softmax в качестве нелинейности на последнем слое, чтобы получить вероятности классов для каждого объекта:\n",
    "$$\\hat y = softmax(x)  = \\bigl \\{\\frac {exp(x_i)}{\\sum_j exp(x_j)} \\bigr \\}_{i=1}^K, \\quad K - \\text{число классов}$$\n",
    "В этом случае удобно оптимизировать логарифм правдоподобия:\n",
    "$$L(y, \\hat y) = -\\sum_{i=1}^K y_i \\log \\hat y_i \\rightarrow \\min,$$\n",
    "где $y_i=1$, если объект принадлежит $i$-му классу, и 0 иначе. Записанная в таком виде, эта функция потерь совпадает с выражением для кросс-энтропии. Очевидно, что ее также можно переписать через индексацию, если через $y_i$ обозначить класс данного объекта:\n",
    "$$L(y, \\hat y) = - \\log \\hat y_{y_i} \\rightarrow \\min$$\n",
    "В таком виде ее удобно реализовывать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте слой Softmax (без параметров). Метод forward должен вычислять логарифм от softmax, а метод backward - пропускать градиенты. В общем случае в промежуточных вычислениях backward получится трехмерный тензор, однако для нашей конкретной функции потерь все вычисления можно реализовать в матричном виде.  Поэтому мы будем предполагать, что аргумент grad_output - это матрица, у которой в каждой строке только одно ненулевое значение (не обязательно единица)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.misc import logsumexp\n",
    "# use this function instead of np.log(np.sum(np.exp(...))) !\n",
    "# because it is more stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Applies softmax to each row and then applies component-wise log\n",
    "        Input shape: [batch, num_units]\n",
    "        Output shape: [batch, num_units]\n",
    "        \"\"\"\n",
    "        output = input - logsumexp(input, axis=1, keepdims=True)\n",
    "        self.x = input\n",
    "        return output\n",
    "        \n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Propagartes gradients.\n",
    "        Assumes that each row of grad_output contains only 1 \n",
    "        non-zero element\n",
    "        Input shape: [batch, num_units]\n",
    "        Output shape: [batch, num_units]\n",
    "        Do not forget to return [] as second value (grad w.r.t. params)\n",
    "        \"\"\"\n",
    "         # softmax\n",
    "        x_exp = np.exp(self.x)\n",
    "        x_exp_sum = np.sum(x_exp, axis=1, keepdims=True)\n",
    "        probabilities = x_exp / x_exp_sum\n",
    "        return grad_output + probabilities, []\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте функцию потерь и градиенты функции потерь. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crossentropy(activations, target):\n",
    "    \"\"\"\n",
    "    returns negative log-likelihood of target under model represented by\n",
    "    activations (log probabilities of classes)\n",
    "    each arg has shape [batch, num_classes]\n",
    "    output shape: 1 (scalar)\n",
    "    \"\"\"\n",
    "    N = activations.shape[0]\n",
    "    loss = - np.sum(activations*target)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def grad_crossentropy(activations, target):\n",
    "    \"\"\"\n",
    "    returns gradient of negative log-likelihood w.r.t. activations\n",
    "    each arg has shape [batch, num_classes]\n",
    "    output shape: [batch, num-classes]\n",
    "    \n",
    "    hint: this is just one-hot encoding of target vector\n",
    "          multiplied by -1\n",
    "    \"\"\"\n",
    "    return target * -1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, выполните проверку softmax-слоя, используя функцию потерь и ее градиент.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "points = np.linspace(-1, 1, 10*12).reshape([10, 12])\n",
    "target = np.arange(10)\n",
    "mtarget = np.zeros_like(points)\n",
    "mtarget[np.arange(len(points)),target] = 1\n",
    "softmax = Softmax()\n",
    "y = softmax.forward(points)\n",
    "crossentropy(y, mtarget)\n",
    "loss = lambda p: crossentropy(softmax.forward(p), mtarget)\n",
    "grads = softmax.backward(grad_crossentropy(y, mtarget))[0]\n",
    "numeric_grads = eval_numerical_gradient(loss, points) \n",
    "\n",
    "assert np.allclose(grads, numeric_grads, rtol=1e-3, atol=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка данных\n",
    "Мы реализаовали все архитектурные составляющие нашей нейронной сети. Осталось загрузить данные и обучить модель. Мы будем работать с датасетом digits, каждый объект в котором - это 8x8 изображение рукописной цифры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = load_digits(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1797, 64), (1797,))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим данные на обучение и контроль:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1347, 64), (450, 64))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сборка и обучение нейронной сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В нашей реализации нейросеть - это список слоев. Например:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "network = []\n",
    "hidden_layers_size = 32\n",
    "network.append(Dense(X_train.shape[1], hidden_layers_size))\n",
    "network.append(ReLU())\n",
    "network.append(Dense(hidden_layers_size, hidden_layers_size))\n",
    "network.append(ReLU())\n",
    "network.append(Dense(hidden_layers_size, 10))\n",
    "network.append(Softmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для проверки, хорошо ли сеть обучилась, нам понадобится вычислять точность (accuracy) на данной выборке. Для этого реализуйте функцию, которая делает предсказания на каждом объекте:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(network, X):\n",
    "    \"\"\"\n",
    "    returns predictions for each object in X\n",
    "    network: list of layers\n",
    "    X: raw data\n",
    "    X shape: [batch, features_num]\n",
    "    output: array of classes, each from 0 to 9\n",
    "    output shape: [batch]\n",
    "    \"\"\"\n",
    "    x = X\n",
    "    for l in network:\n",
    "        x = l.forward(x)\n",
    "    return np.argmax(x, axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы будем обучать параметры нейросети с помощью готовой функции оптимизации из модуля scipy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function minimize in module scipy.optimize._minimize:\n",
      "\n",
      "minimize(fun, x0, args=(), method=None, jac=None, hess=None, hessp=None, bounds=None, constraints=(), tol=None, callback=None, options=None)\n",
      "    Minimization of scalar function of one or more variables.\n",
      "    \n",
      "    In general, the optimization problems are of the form::\n",
      "    \n",
      "        minimize f(x) subject to\n",
      "    \n",
      "        g_i(x) >= 0,  i = 1,...,m\n",
      "        h_j(x)  = 0,  j = 1,...,p\n",
      "    \n",
      "    where x is a vector of one or more variables.\n",
      "    ``g_i(x)`` are the inequality constraints.\n",
      "    ``h_j(x)`` are the equality constrains.\n",
      "    \n",
      "    Optionally, the lower and upper bounds for each element in x can also be\n",
      "    specified using the `bounds` argument.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    fun : callable\n",
      "        Objective function.\n",
      "    x0 : ndarray\n",
      "        Initial guess.\n",
      "    args : tuple, optional\n",
      "        Extra arguments passed to the objective function and its\n",
      "        derivatives (Jacobian, Hessian).\n",
      "    method : str or callable, optional\n",
      "        Type of solver.  Should be one of\n",
      "    \n",
      "            - 'Nelder-Mead' :ref:`(see here) <optimize.minimize-neldermead>`\n",
      "            - 'Powell'      :ref:`(see here) <optimize.minimize-powell>`\n",
      "            - 'CG'          :ref:`(see here) <optimize.minimize-cg>`\n",
      "            - 'BFGS'        :ref:`(see here) <optimize.minimize-bfgs>`\n",
      "            - 'Newton-CG'   :ref:`(see here) <optimize.minimize-newtoncg>`\n",
      "            - 'L-BFGS-B'    :ref:`(see here) <optimize.minimize-lbfgsb>`\n",
      "            - 'TNC'         :ref:`(see here) <optimize.minimize-tnc>`\n",
      "            - 'COBYLA'      :ref:`(see here) <optimize.minimize-cobyla>`\n",
      "            - 'SLSQP'       :ref:`(see here) <optimize.minimize-slsqp>`\n",
      "            - 'dogleg'      :ref:`(see here) <optimize.minimize-dogleg>`\n",
      "            - 'trust-ncg'   :ref:`(see here) <optimize.minimize-trustncg>`\n",
      "            - custom - a callable object (added in version 0.14.0),\n",
      "              see below for description.\n",
      "    \n",
      "        If not given, chosen to be one of ``BFGS``, ``L-BFGS-B``, ``SLSQP``,\n",
      "        depending if the problem has constraints or bounds.\n",
      "    jac : bool or callable, optional\n",
      "        Jacobian (gradient) of objective function. Only for CG, BFGS,\n",
      "        Newton-CG, L-BFGS-B, TNC, SLSQP, dogleg, trust-ncg.\n",
      "        If `jac` is a Boolean and is True, `fun` is assumed to return the\n",
      "        gradient along with the objective function. If False, the\n",
      "        gradient will be estimated numerically.\n",
      "        `jac` can also be a callable returning the gradient of the\n",
      "        objective. In this case, it must accept the same arguments as `fun`.\n",
      "    hess, hessp : callable, optional\n",
      "        Hessian (matrix of second-order derivatives) of objective function or\n",
      "        Hessian of objective function times an arbitrary vector p.  Only for\n",
      "        Newton-CG, dogleg, trust-ncg.\n",
      "        Only one of `hessp` or `hess` needs to be given.  If `hess` is\n",
      "        provided, then `hessp` will be ignored.  If neither `hess` nor\n",
      "        `hessp` is provided, then the Hessian product will be approximated\n",
      "        using finite differences on `jac`. `hessp` must compute the Hessian\n",
      "        times an arbitrary vector.\n",
      "    bounds : sequence, optional\n",
      "        Bounds for variables (only for L-BFGS-B, TNC and SLSQP).\n",
      "        ``(min, max)`` pairs for each element in ``x``, defining\n",
      "        the bounds on that parameter. Use None for one of ``min`` or\n",
      "        ``max`` when there is no bound in that direction.\n",
      "    constraints : dict or sequence of dict, optional\n",
      "        Constraints definition (only for COBYLA and SLSQP).\n",
      "        Each constraint is defined in a dictionary with fields:\n",
      "    \n",
      "            type : str\n",
      "                Constraint type: 'eq' for equality, 'ineq' for inequality.\n",
      "            fun : callable\n",
      "                The function defining the constraint.\n",
      "            jac : callable, optional\n",
      "                The Jacobian of `fun` (only for SLSQP).\n",
      "            args : sequence, optional\n",
      "                Extra arguments to be passed to the function and Jacobian.\n",
      "    \n",
      "        Equality constraint means that the constraint function result is to\n",
      "        be zero whereas inequality means that it is to be non-negative.\n",
      "        Note that COBYLA only supports inequality constraints.\n",
      "    tol : float, optional\n",
      "        Tolerance for termination. For detailed control, use solver-specific\n",
      "        options.\n",
      "    options : dict, optional\n",
      "        A dictionary of solver options. All methods accept the following\n",
      "        generic options:\n",
      "    \n",
      "            maxiter : int\n",
      "                Maximum number of iterations to perform.\n",
      "            disp : bool\n",
      "                Set to True to print convergence messages.\n",
      "    \n",
      "        For method-specific options, see :func:`show_options()`.\n",
      "    callback : callable, optional\n",
      "        Called after each iteration, as ``callback(xk)``, where ``xk`` is the\n",
      "        current parameter vector.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    res : OptimizeResult\n",
      "        The optimization result represented as a ``OptimizeResult`` object.\n",
      "        Important attributes are: ``x`` the solution array, ``success`` a\n",
      "        Boolean flag indicating if the optimizer exited successfully and\n",
      "        ``message`` which describes the cause of the termination. See\n",
      "        `OptimizeResult` for a description of other attributes.\n",
      "    \n",
      "    \n",
      "    See also\n",
      "    --------\n",
      "    minimize_scalar : Interface to minimization algorithms for scalar\n",
      "        univariate functions\n",
      "    show_options : Additional options accepted by the solvers\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    This section describes the available solvers that can be selected by the\n",
      "    'method' parameter. The default method is *BFGS*.\n",
      "    \n",
      "    **Unconstrained minimization**\n",
      "    \n",
      "    Method :ref:`Nelder-Mead <optimize.minimize-neldermead>` uses the\n",
      "    Simplex algorithm [1]_, [2]_. This algorithm is robust in many\n",
      "    applications. However, if numerical computation of derivative can be\n",
      "    trusted, other algorithms using the first and/or second derivatives\n",
      "    information might be preferred for their better performance in\n",
      "    general.\n",
      "    \n",
      "    Method :ref:`Powell <optimize.minimize-powell>` is a modification\n",
      "    of Powell's method [3]_, [4]_ which is a conjugate direction\n",
      "    method. It performs sequential one-dimensional minimizations along\n",
      "    each vector of the directions set (`direc` field in `options` and\n",
      "    `info`), which is updated at each iteration of the main\n",
      "    minimization loop. The function need not be differentiable, and no\n",
      "    derivatives are taken.\n",
      "    \n",
      "    Method :ref:`CG <optimize.minimize-cg>` uses a nonlinear conjugate\n",
      "    gradient algorithm by Polak and Ribiere, a variant of the\n",
      "    Fletcher-Reeves method described in [5]_ pp.  120-122. Only the\n",
      "    first derivatives are used.\n",
      "    \n",
      "    Method :ref:`BFGS <optimize.minimize-bfgs>` uses the quasi-Newton\n",
      "    method of Broyden, Fletcher, Goldfarb, and Shanno (BFGS) [5]_\n",
      "    pp. 136. It uses the first derivatives only. BFGS has proven good\n",
      "    performance even for non-smooth optimizations. This method also\n",
      "    returns an approximation of the Hessian inverse, stored as\n",
      "    `hess_inv` in the OptimizeResult object.\n",
      "    \n",
      "    Method :ref:`Newton-CG <optimize.minimize-newtoncg>` uses a\n",
      "    Newton-CG algorithm [5]_ pp. 168 (also known as the truncated\n",
      "    Newton method). It uses a CG method to the compute the search\n",
      "    direction. See also *TNC* method for a box-constrained\n",
      "    minimization with a similar algorithm.\n",
      "    \n",
      "    Method :ref:`dogleg <optimize.minimize-dogleg>` uses the dog-leg\n",
      "    trust-region algorithm [5]_ for unconstrained minimization. This\n",
      "    algorithm requires the gradient and Hessian; furthermore the\n",
      "    Hessian is required to be positive definite.\n",
      "    \n",
      "    Method :ref:`trust-ncg <optimize.minimize-trustncg>` uses the\n",
      "    Newton conjugate gradient trust-region algorithm [5]_ for\n",
      "    unconstrained minimization. This algorithm requires the gradient\n",
      "    and either the Hessian or a function that computes the product of\n",
      "    the Hessian with a given vector.\n",
      "    \n",
      "    **Constrained minimization**\n",
      "    \n",
      "    Method :ref:`L-BFGS-B <optimize.minimize-lbfgsb>` uses the L-BFGS-B\n",
      "    algorithm [6]_, [7]_ for bound constrained minimization.\n",
      "    \n",
      "    Method :ref:`TNC <optimize.minimize-tnc>` uses a truncated Newton\n",
      "    algorithm [5]_, [8]_ to minimize a function with variables subject\n",
      "    to bounds. This algorithm uses gradient information; it is also\n",
      "    called Newton Conjugate-Gradient. It differs from the *Newton-CG*\n",
      "    method described above as it wraps a C implementation and allows\n",
      "    each variable to be given upper and lower bounds.\n",
      "    \n",
      "    Method :ref:`COBYLA <optimize.minimize-cobyla>` uses the\n",
      "    Constrained Optimization BY Linear Approximation (COBYLA) method\n",
      "    [9]_, [10]_, [11]_. The algorithm is based on linear\n",
      "    approximations to the objective function and each constraint. The\n",
      "    method wraps a FORTRAN implementation of the algorithm. The\n",
      "    constraints functions 'fun' may return either a single number\n",
      "    or an array or list of numbers.\n",
      "    \n",
      "    Method :ref:`SLSQP <optimize.minimize-slsqp>` uses Sequential\n",
      "    Least SQuares Programming to minimize a function of several\n",
      "    variables with any combination of bounds, equality and inequality\n",
      "    constraints. The method wraps the SLSQP Optimization subroutine\n",
      "    originally implemented by Dieter Kraft [12]_. Note that the\n",
      "    wrapper handles infinite values in bounds by converting them into\n",
      "    large floating values.\n",
      "    \n",
      "    **Custom minimizers**\n",
      "    \n",
      "    It may be useful to pass a custom minimization method, for example\n",
      "    when using a frontend to this method such as `scipy.optimize.basinhopping`\n",
      "    or a different library.  You can simply pass a callable as the ``method``\n",
      "    parameter.\n",
      "    \n",
      "    The callable is called as ``method(fun, x0, args, **kwargs, **options)``\n",
      "    where ``kwargs`` corresponds to any other parameters passed to `minimize`\n",
      "    (such as `callback`, `hess`, etc.), except the `options` dict, which has\n",
      "    its contents also passed as `method` parameters pair by pair.  Also, if\n",
      "    `jac` has been passed as a bool type, `jac` and `fun` are mangled so that\n",
      "    `fun` returns just the function values and `jac` is converted to a function\n",
      "    returning the Jacobian.  The method shall return an ``OptimizeResult``\n",
      "    object.\n",
      "    \n",
      "    The provided `method` callable must be able to accept (and possibly ignore)\n",
      "    arbitrary parameters; the set of parameters accepted by `minimize` may\n",
      "    expand in future versions and then these parameters will be passed to\n",
      "    the method.  You can find an example in the scipy.optimize tutorial.\n",
      "    \n",
      "    .. versionadded:: 0.11.0\n",
      "    \n",
      "    References\n",
      "    ----------\n",
      "    .. [1] Nelder, J A, and R Mead. 1965. A Simplex Method for Function\n",
      "        Minimization. The Computer Journal 7: 308-13.\n",
      "    .. [2] Wright M H. 1996. Direct search methods: Once scorned, now\n",
      "        respectable, in Numerical Analysis 1995: Proceedings of the 1995\n",
      "        Dundee Biennial Conference in Numerical Analysis (Eds. D F\n",
      "        Griffiths and G A Watson). Addison Wesley Longman, Harlow, UK.\n",
      "        191-208.\n",
      "    .. [3] Powell, M J D. 1964. An efficient method for finding the minimum of\n",
      "       a function of several variables without calculating derivatives. The\n",
      "       Computer Journal 7: 155-162.\n",
      "    .. [4] Press W, S A Teukolsky, W T Vetterling and B P Flannery.\n",
      "       Numerical Recipes (any edition), Cambridge University Press.\n",
      "    .. [5] Nocedal, J, and S J Wright. 2006. Numerical Optimization.\n",
      "       Springer New York.\n",
      "    .. [6] Byrd, R H and P Lu and J. Nocedal. 1995. A Limited Memory\n",
      "       Algorithm for Bound Constrained Optimization. SIAM Journal on\n",
      "       Scientific and Statistical Computing 16 (5): 1190-1208.\n",
      "    .. [7] Zhu, C and R H Byrd and J Nocedal. 1997. L-BFGS-B: Algorithm\n",
      "       778: L-BFGS-B, FORTRAN routines for large scale bound constrained\n",
      "       optimization. ACM Transactions on Mathematical Software 23 (4):\n",
      "       550-560.\n",
      "    .. [8] Nash, S G. Newton-Type Minimization Via the Lanczos Method.\n",
      "       1984. SIAM Journal of Numerical Analysis 21: 770-778.\n",
      "    .. [9] Powell, M J D. A direct search optimization method that models\n",
      "       the objective and constraint functions by linear interpolation.\n",
      "       1994. Advances in Optimization and Numerical Analysis, eds. S. Gomez\n",
      "       and J-P Hennart, Kluwer Academic (Dordrecht), 51-67.\n",
      "    .. [10] Powell M J D. Direct search algorithms for optimization\n",
      "       calculations. 1998. Acta Numerica 7: 287-336.\n",
      "    .. [11] Powell M J D. A view of algorithms for optimization without\n",
      "       derivatives. 2007.Cambridge University Technical Report DAMTP\n",
      "       2007/NA03\n",
      "    .. [12] Kraft, D. A software package for sequential quadratic\n",
      "       programming. 1988. Tech. Rep. DFVLR-FB 88-28, DLR German Aerospace\n",
      "       Center -- Institute for Flight Mechanics, Koln, Germany.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    Let us consider the problem of minimizing the Rosenbrock function. This\n",
      "    function (and its respective derivatives) is implemented in `rosen`\n",
      "    (resp. `rosen_der`, `rosen_hess`) in the `scipy.optimize`.\n",
      "    \n",
      "    >>> from scipy.optimize import minimize, rosen, rosen_der\n",
      "    \n",
      "    A simple application of the *Nelder-Mead* method is:\n",
      "    \n",
      "    >>> x0 = [1.3, 0.7, 0.8, 1.9, 1.2]\n",
      "    >>> res = minimize(rosen, x0, method='Nelder-Mead', tol=1e-6)\n",
      "    >>> res.x\n",
      "    array([ 1.,  1.,  1.,  1.,  1.])\n",
      "    \n",
      "    Now using the *BFGS* algorithm, using the first derivative and a few\n",
      "    options:\n",
      "    \n",
      "    >>> res = minimize(rosen, x0, method='BFGS', jac=rosen_der,\n",
      "    ...                options={'gtol': 1e-6, 'disp': True})\n",
      "    Optimization terminated successfully.\n",
      "             Current function value: 0.000000\n",
      "             Iterations: 26\n",
      "             Function evaluations: 31\n",
      "             Gradient evaluations: 31\n",
      "    >>> res.x\n",
      "    array([ 1.,  1.,  1.,  1.,  1.])\n",
      "    >>> print(res.message)\n",
      "    Optimization terminated successfully.\n",
      "    >>> res.hess_inv\n",
      "    array([[ 0.00749589,  0.01255155,  0.02396251,  0.04750988,  0.09495377],  # may vary\n",
      "           [ 0.01255155,  0.02510441,  0.04794055,  0.09502834,  0.18996269],\n",
      "           [ 0.02396251,  0.04794055,  0.09631614,  0.19092151,  0.38165151],\n",
      "           [ 0.04750988,  0.09502834,  0.19092151,  0.38341252,  0.7664427 ],\n",
      "           [ 0.09495377,  0.18996269,  0.38165151,  0.7664427,   1.53713523]])\n",
      "    \n",
      "    \n",
      "    Next, consider a minimization problem with several constraints (namely\n",
      "    Example 16.4 from [5]_). The objective function is:\n",
      "    \n",
      "    >>> fun = lambda x: (x[0] - 1)**2 + (x[1] - 2.5)**2\n",
      "    \n",
      "    There are three constraints defined as:\n",
      "    \n",
      "    >>> cons = ({'type': 'ineq', 'fun': lambda x:  x[0] - 2 * x[1] + 2},\n",
      "    ...         {'type': 'ineq', 'fun': lambda x: -x[0] - 2 * x[1] + 6},\n",
      "    ...         {'type': 'ineq', 'fun': lambda x: -x[0] + 2 * x[1] + 2})\n",
      "    \n",
      "    And variables must be positive, hence the following bounds:\n",
      "    \n",
      "    >>> bnds = ((0, None), (0, None))\n",
      "    \n",
      "    The optimization problem is solved using the SLSQP method as:\n",
      "    \n",
      "    >>> res = minimize(fun, (2, 0), method='SLSQP', bounds=bnds,\n",
      "    ...                constraints=cons)\n",
      "    \n",
      "    It should converge to the theoretical solution (1.4 ,1.7).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(minimize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эта функция имеет стандартный интерфейс: нужно передать callable объект, который вычисляет значение и градиент целевой функции, а также точку старта оптимизации - начальное приближение (одномерный numpy-массив). Поэтому нам понадобятся функции для сбора и задания всех весов нашей нейросети (именно для них мы всегда записывали параметры слоя в список layer.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_weights(network):\n",
    "    weights = []\n",
    "    for layer in network:\n",
    "        for param in layer.params:\n",
    "            weights += param.ravel().tolist()\n",
    "    return np.array(weights)\n",
    "\n",
    "def set_weights(weights, network):\n",
    "    i = 0\n",
    "    for layer in network:\n",
    "        for param in layer.params:\n",
    "            l = param.size\n",
    "            param[:] = weights[i:i+l].\\\n",
    "                             reshape(param.shape)\n",
    "            i += l\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вам нужно реализовать ту самую функцию, которую мы будем передавать в minimize. Эта функция должна брать на вход текущую точку (вектор всех параметров), а также список дополнительных параметров (мы будем передавать через них нашу сеть и обучающие данные) и возвращать значение критерия качества (кросс-энтропия) и его градиент по параметрам модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_loss_grad(weights, args):\n",
    "    \"\"\"\n",
    "    takes current weights and computes cross-entropy and gradients\n",
    "    weights shape: [num_parameters]\n",
    "    output 1: loss (scalar)\n",
    "    output 2: gradint w.r.t. weights, shape: [num_parameters]\n",
    "    \n",
    "    hint: firstly perform forward pass through the whole network\n",
    "    then compute loss and its gradients\n",
    "    then perform backward pass, transmitting first backward output\n",
    "    to the previos layer and saving second backward output in a list\n",
    "    finally flatten all the gradients in this list\n",
    "    (in the order from the first to the last layer)\n",
    "    \n",
    "    Do not forget to set weights of the network!\n",
    "    \"\"\"\n",
    "    network, X, y = args\n",
    "    set_weights(weights, network)\n",
    "    x = X\n",
    "    for l in network:\n",
    "        x = l.forward(x)\n",
    "        \n",
    "    my = np.zeros_like(x)\n",
    "    my[np.arange(len(x)), y] = 1\n",
    "    \n",
    "    loss = crossentropy(x, my)\n",
    "    dloss = grad_crossentropy(x, my)\n",
    "    \n",
    "    grad_output = dloss\n",
    "    params = []\n",
    "    for l in reversed(network):\n",
    "        grad_output, param = l.backward(grad_output)\n",
    "        params.append(param)\n",
    "        \n",
    "    params = reversed(params)\n",
    "    flatten_params = np.array([item for par in params for item in par])\n",
    "    return loss, flatten_params\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы готовы обучать нашу нейросеть. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = get_weights(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = minimize(compute_loss_grad, weights,  # fun and start point\n",
    "               args=[network, X_train, y_train], # args passed to fun\n",
    "               method=\"L-BFGS-B\", # optimization method\n",
    "               jac=True) # says that gradient are computed in fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['fun', 'jac', 'nfev', 'nit', 'status', 'message', 'x', 'success', 'hess_inv'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"nit\"] # number of iterations (should be >> 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"success\"] # should be True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00472387, -0.0039238 ,  0.01275838, ..., -1.06551648,\n",
       "       -2.81310987, -0.5219706 ])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"x\"] # learned weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведите качество на обучении (X_train, y_train) и на контроле (X_test, y_test). Не забудьте установить веса!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9577777777777777\n"
     ]
    }
   ],
   "source": [
    "set_weights(res[\"x\"], network)\n",
    "\n",
    "y_pred = predict(network, X_train)\n",
    "y_pred_test = predict(network, X_test)\n",
    "\n",
    "train_accuracy = np.count_nonzero(y_pred == y_train) / len(y_train)\n",
    "test_accuracy = np.count_nonzero(y_pred_test == y_test) / len(y_test)\n",
    "print(\"Train accuracy: \", train_accuracy)\n",
    "print(\"Test accuracy: \", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У minimize есть также аргумент callback - в нее можно передать функцию, которая будет вызываться после каждой итерации оптимизации. Такую функцию удобно оформить в виде метода класса, который будет сохранять качество на обучении контроле после каждой итерации. Реализуйте этот метод в классе Callback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Callback:\n",
    "    def __init__(self, network, X_train, y_train, X_test, y_test, print=False):\n",
    "        self.network = network\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "        self.print = print\n",
    "        self.train_acc = []\n",
    "        self.test_acc = []\n",
    "        \n",
    "    def call(self, weights):\n",
    "        \"\"\"\n",
    "        computes quality on train and test set with given weights\n",
    "        and saves to self.train_acc and self.test_acc\n",
    "        if self.print is True, also prints these 2 values\n",
    "        \"\"\"\n",
    "        set_weights(weights, self.network)\n",
    "\n",
    "        y_pred = predict(self.network, self.X_train)\n",
    "        y_pred_test = predict(self.network, self.X_test)\n",
    "        \n",
    "        train_accuracy = np.count_nonzero(y_pred == self.y_train) / len(self.y_train)\n",
    "        test_accuracy = np.count_nonzero(y_pred_test == self.y_test) / len(self.y_test)\n",
    "        \n",
    "        self.train_acc.append(train_accuracy)\n",
    "        self.test_acc.append(test_accuracy)\n",
    "        \n",
    "        if self.print:\n",
    "            print(\"Train accuracy: \", train_accuracy)\n",
    "            print(\"Test accuracy: \", test_accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.17520415738678544\n",
      "Test accuracy:  0.16444444444444445\n",
      "Train accuracy:  0.2071269487750557\n",
      "Test accuracy:  0.18222222222222223\n",
      "Train accuracy:  0.2056421677802524\n",
      "Test accuracy:  0.18222222222222223\n",
      "Train accuracy:  0.19376391982182628\n",
      "Test accuracy:  0.17333333333333334\n",
      "Train accuracy:  0.19821826280623608\n",
      "Test accuracy:  0.17555555555555555\n",
      "Train accuracy:  0.24944320712694878\n",
      "Test accuracy:  0.23555555555555555\n",
      "Train accuracy:  0.20861172976985895\n",
      "Test accuracy:  0.22\n",
      "Train accuracy:  0.22345953971789162\n",
      "Test accuracy:  0.24444444444444444\n",
      "Train accuracy:  0.36971046770601335\n",
      "Test accuracy:  0.38666666666666666\n",
      "Train accuracy:  0.4766146993318486\n",
      "Test accuracy:  0.4577777777777778\n",
      "Train accuracy:  0.4974016332590943\n",
      "Test accuracy:  0.4488888888888889\n",
      "Train accuracy:  0.5412026726057907\n",
      "Test accuracy:  0.5222222222222223\n",
      "Train accuracy:  0.5493689680772086\n",
      "Test accuracy:  0.5466666666666666\n",
      "Train accuracy:  0.6755753526354863\n",
      "Test accuracy:  0.6733333333333333\n",
      "Train accuracy:  0.6755753526354863\n",
      "Test accuracy:  0.6711111111111111\n",
      "Train accuracy:  0.6577579806978471\n",
      "Test accuracy:  0.6533333333333333\n",
      "Train accuracy:  0.7052709725315516\n",
      "Test accuracy:  0.6933333333333334\n",
      "Train accuracy:  0.7461024498886414\n",
      "Test accuracy:  0.7377777777777778\n",
      "Train accuracy:  0.7661469933184856\n",
      "Test accuracy:  0.7733333333333333\n",
      "Train accuracy:  0.7557535263548627\n",
      "Test accuracy:  0.7444444444444445\n",
      "Train accuracy:  0.7809948032665182\n",
      "Test accuracy:  0.7533333333333333\n",
      "Train accuracy:  0.7884187082405345\n",
      "Test accuracy:  0.7622222222222222\n",
      "Train accuracy:  0.7980697847067557\n",
      "Test accuracy:  0.7733333333333333\n",
      "Train accuracy:  0.8069784706755754\n",
      "Test accuracy:  0.78\n",
      "Train accuracy:  0.8121752041573868\n",
      "Test accuracy:  0.8088888888888889\n",
      "Train accuracy:  0.8277654046028211\n",
      "Test accuracy:  0.8222222222222222\n",
      "Train accuracy:  0.8389012620638456\n",
      "Test accuracy:  0.8288888888888889\n",
      "Train accuracy:  0.85003711952487\n",
      "Test accuracy:  0.8355555555555556\n",
      "Train accuracy:  0.852264291017075\n",
      "Test accuracy:  0.8688888888888889\n",
      "Train accuracy:  0.8596881959910914\n",
      "Test accuracy:  0.8688888888888889\n",
      "Train accuracy:  0.8663697104677061\n",
      "Test accuracy:  0.8711111111111111\n",
      "Train accuracy:  0.8752783964365256\n",
      "Test accuracy:  0.8777777777777778\n",
      "Train accuracy:  0.8767631774313289\n",
      "Test accuracy:  0.8844444444444445\n",
      "Train accuracy:  0.882702301410542\n",
      "Test accuracy:  0.8933333333333333\n",
      "Train accuracy:  0.8916109873793615\n",
      "Test accuracy:  0.8911111111111111\n",
      "Train accuracy:  0.8953229398663697\n",
      "Test accuracy:  0.9066666666666666\n",
      "Train accuracy:  0.9012620638455828\n",
      "Test accuracy:  0.9088888888888889\n",
      "Train accuracy:  0.902746844840386\n",
      "Test accuracy:  0.9111111111111111\n",
      "Train accuracy:  0.9012620638455828\n",
      "Test accuracy:  0.9044444444444445\n",
      "Train accuracy:  0.9123979213066072\n",
      "Test accuracy:  0.9066666666666666\n",
      "Train accuracy:  0.910913140311804\n",
      "Test accuracy:  0.9155555555555556\n",
      "Train accuracy:  0.9205642167780252\n",
      "Test accuracy:  0.9111111111111111\n",
      "Train accuracy:  0.9205642167780252\n",
      "Test accuracy:  0.9\n",
      "Train accuracy:  0.9279881217520416\n",
      "Test accuracy:  0.9044444444444445\n",
      "Train accuracy:  0.9287305122494433\n",
      "Test accuracy:  0.9133333333333333\n",
      "Train accuracy:  0.9383815887156645\n",
      "Test accuracy:  0.9244444444444444\n",
      "Train accuracy:  0.947290274684484\n",
      "Test accuracy:  0.9133333333333333\n",
      "Train accuracy:  0.949517446176689\n",
      "Test accuracy:  0.9177777777777778\n",
      "Train accuracy:  0.9502598366740905\n",
      "Test accuracy:  0.9222222222222223\n",
      "Train accuracy:  0.9584261321455085\n",
      "Test accuracy:  0.9177777777777778\n",
      "Train accuracy:  0.9576837416481069\n",
      "Test accuracy:  0.92\n",
      "Train accuracy:  0.9658500371195249\n",
      "Test accuracy:  0.9222222222222223\n",
      "Train accuracy:  0.9658500371195249\n",
      "Test accuracy:  0.9311111111111111\n",
      "Train accuracy:  0.9665924276169265\n",
      "Test accuracy:  0.9355555555555556\n",
      "Train accuracy:  0.9643652561247216\n",
      "Test accuracy:  0.9333333333333333\n",
      "Train accuracy:  0.9680772086117297\n",
      "Test accuracy:  0.9333333333333333\n",
      "Train accuracy:  0.9695619896065331\n",
      "Test accuracy:  0.94\n",
      "Train accuracy:  0.9703043801039347\n",
      "Test accuracy:  0.9444444444444444\n",
      "Train accuracy:  0.9755011135857461\n",
      "Test accuracy:  0.9466666666666667\n",
      "Train accuracy:  0.9784706755753526\n",
      "Test accuracy:  0.9444444444444444\n",
      "Train accuracy:  0.9784706755753526\n",
      "Test accuracy:  0.9466666666666667\n",
      "Train accuracy:  0.9769858945805494\n",
      "Test accuracy:  0.9488888888888889\n",
      "Train accuracy:  0.9799554565701559\n",
      "Test accuracy:  0.9511111111111111\n",
      "Train accuracy:  0.9829250185597624\n",
      "Test accuracy:  0.9444444444444444\n",
      "Train accuracy:  0.985894580549369\n",
      "Test accuracy:  0.9466666666666667\n",
      "Train accuracy:  0.9873793615441723\n",
      "Test accuracy:  0.9466666666666667\n",
      "Train accuracy:  0.9866369710467706\n",
      "Test accuracy:  0.94\n",
      "Train accuracy:  0.9866369710467706\n",
      "Test accuracy:  0.9422222222222222\n",
      "Train accuracy:  0.985894580549369\n",
      "Test accuracy:  0.9422222222222222\n",
      "Train accuracy:  0.9866369710467706\n",
      "Test accuracy:  0.9488888888888889\n",
      "Train accuracy:  0.9888641425389755\n",
      "Test accuracy:  0.9466666666666667\n",
      "Train accuracy:  0.9896065330363771\n",
      "Test accuracy:  0.9511111111111111\n",
      "Train accuracy:  0.9896065330363771\n",
      "Test accuracy:  0.9511111111111111\n",
      "Train accuracy:  0.9896065330363771\n",
      "Test accuracy:  0.9488888888888889\n",
      "Train accuracy:  0.9903489235337788\n",
      "Test accuracy:  0.9511111111111111\n",
      "Train accuracy:  0.9933184855233853\n",
      "Test accuracy:  0.9555555555555556\n",
      "Train accuracy:  0.9925760950259837\n",
      "Test accuracy:  0.9488888888888889\n",
      "Train accuracy:  0.994060876020787\n",
      "Test accuracy:  0.9466666666666667\n",
      "Train accuracy:  0.9948032665181886\n",
      "Test accuracy:  0.9466666666666667\n",
      "Train accuracy:  0.994060876020787\n",
      "Test accuracy:  0.9466666666666667\n",
      "Train accuracy:  0.994060876020787\n",
      "Test accuracy:  0.9511111111111111\n",
      "Train accuracy:  0.9933184855233853\n",
      "Test accuracy:  0.9466666666666667\n",
      "Train accuracy:  0.9962880475129918\n",
      "Test accuracy:  0.9511111111111111\n",
      "Train accuracy:  0.9955456570155902\n",
      "Test accuracy:  0.9533333333333334\n",
      "Train accuracy:  0.9955456570155902\n",
      "Test accuracy:  0.9511111111111111\n",
      "Train accuracy:  0.9955456570155902\n",
      "Test accuracy:  0.9511111111111111\n",
      "Train accuracy:  0.9955456570155902\n",
      "Test accuracy:  0.9488888888888889\n",
      "Train accuracy:  0.9970304380103935\n",
      "Test accuracy:  0.9488888888888889\n",
      "Train accuracy:  0.9970304380103935\n",
      "Test accuracy:  0.9488888888888889\n",
      "Train accuracy:  0.9977728285077951\n",
      "Test accuracy:  0.9488888888888889\n",
      "Train accuracy:  0.9977728285077951\n",
      "Test accuracy:  0.9466666666666667\n",
      "Train accuracy:  0.9977728285077951\n",
      "Test accuracy:  0.9466666666666667\n",
      "Train accuracy:  0.9985152190051967\n",
      "Test accuracy:  0.9444444444444444\n",
      "Train accuracy:  0.9992576095025983\n",
      "Test accuracy:  0.9444444444444444\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9466666666666667\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9466666666666667\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9444444444444444\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9466666666666667\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9488888888888889\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9466666666666667\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9466666666666667\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9466666666666667\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9444444444444444\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9466666666666667\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9466666666666667\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9488888888888889\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9511111111111111\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9488888888888889\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9488888888888889\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9488888888888889\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9511111111111111\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9511111111111111\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9511111111111111\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9488888888888889\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9488888888888889\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9511111111111111\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9466666666666667\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9466666666666667\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9488888888888889\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9511111111111111\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9511111111111111\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9533333333333334\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9533333333333334\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9511111111111111\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9511111111111111\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9488888888888889\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9511111111111111\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9511111111111111\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9511111111111111\n",
      "Train accuracy:  1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.9511111111111111\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9511111111111111\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9511111111111111\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9511111111111111\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9511111111111111\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9511111111111111\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9511111111111111\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9533333333333334\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9533333333333334\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9555555555555556\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9555555555555556\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9533333333333334\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9511111111111111\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9511111111111111\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9488888888888889\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9488888888888889\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9488888888888889\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9488888888888889\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9488888888888889\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9488888888888889\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9511111111111111\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9511111111111111\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9511111111111111\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9533333333333334\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9533333333333334\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9533333333333334\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9533333333333334\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9555555555555556\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9555555555555556\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9577777777777777\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.96\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.96\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.96\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.96\n",
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9577777777777777\n"
     ]
    }
   ],
   "source": [
    "cb = Callback(network, X_train, y_train, X_test, y_test, print=True)\n",
    "res = minimize(compute_loss_grad, weights,  \n",
    "               args=[network, X_train, y_train], \n",
    "               method=\"L-BFGS-B\",\n",
    "               jac=True,\n",
    "               callback=cb.call)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изобразите на графике кривую качества на обучени ии контроле по итерациям:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x10c33b3c8>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFioKIiLS4v8DSTH1SsDL1woAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118a54898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cb.train_acc, label=\"train acc\")\n",
    "plt.plot(cb.test_acc, label=\"test acc\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Эксперименты с числом слоев"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ясно, что из-за случайного начального приближения с каждым запуском обучения мы будем получать различное качество. Попробуем обучать нашу нейросеть с разным числом слоев несколько раз.\n",
    "\n",
    "Заполните матрицы accs_train и accs_test. В позиции [i, j] должна стоять величина точности сети с $i+1$ полносвязными слоями при $j$-м запуске (все запуски идентичны)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accs_train = np.zeros((5, 5))\n",
    "accs_test = np.zeros((5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "hidden_layers_size = 32\n",
    "for j in range(5):\n",
    "    for i in range(5):\n",
    "        networks = []\n",
    "        \n",
    "        if i == 0:  \n",
    "            networks.append(Dense(X_train.shape[1], 10))\n",
    "        else:\n",
    "            networks.append(Dense(X_train.shape[1], hidden_layers_size))\n",
    "            networks.append(ReLU())\n",
    "            \n",
    "        for k in range(i):\n",
    "            if k == i-1:\n",
    "                networks.append(Dense(hidden_layers_size, 10))\n",
    "            else:\n",
    "                networks.append(Dense(hidden_layers_size, hidden_layers_size))\n",
    "                networks.append(ReLU())\n",
    "       \n",
    "        networks.append(Softmax())\n",
    "#         print(i,j, len(networks))\n",
    "        weights = get_weights(networks)\n",
    "\n",
    "        cb = Callback(networks, X_train, y_train, X_test, y_test, print=False)\n",
    "        res = minimize(compute_loss_grad, weights,  \n",
    "                       args=[networks, X_train, y_train], \n",
    "                       method=\"L-BFGS-B\",\n",
    "                       jac=True,\n",
    "                       callback=cb.call)\n",
    "        set_weights(res[\"x\"], networks)\n",
    "\n",
    "        y_pred = predict(networks, X_train)\n",
    "        y_pred_test = predict(networks, X_test)\n",
    "\n",
    "        accs_train[i, j] = max(cb.train_acc)#np.count_nonzero(y_pred == y_train) / len(y_train)\n",
    "        accs_test[i, j] = max(cb.test_acc)#np.count_nonzero(y_pred_test == y_test) / len(y_test)\n",
    "#         print(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим боксплоты полученного качества (горизонтальная линия в каждом столбце - среднее, прямоугольник показывает разброс)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x118c0ee48>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZShIklqGgiSpZShIklr/H2pQr7WVn2uNAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118b8f4a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.boxplot(accs_test.T, showfliers=False)\n",
    "plt.xlabel(\"Num layers\")\n",
    "plt.ylabel(\"Test accuracy\")\n",
    "plt.title(\"Test quality in 5 runs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ответьте на вопросы (кратко в этой же ячейке):\n",
    "* Как изменяются качество на обучении и контроле и устойчивость процесса обучения при увеличении числа слоев?\n",
    "   * Качество сети возрастает с увеличением числа слоёв и числа нейронов в ней. Количество     нейронов в полносвязном слое - важный параметр сложности сети, от которого может существенно зависеть качество обучения и скорость сходимости. При чрезмерном увеличении числа весов сеть склонна к переобучению. Также необходимо отметить, что качество на обучении и контроле, а также устойчивость процесса во многом зависит от начальной инициализации весов.\n",
    "* Можно ли сказать, что логистическая регрессия (линейная модель) дает качество хуже, чем нелинейная модель?\n",
    "   * Линейная регрессионная модель не всегда способна качественно предсказывать значения зависимой переменной. Однако все зависит прежде всего от данных и различных зависимостей внутри них. Если данные можно разделить линейно, тогда логистическая регрессия даст хороший результат. Если рассматривать общие случаи и оценивать только качество, тогда наверное можно так сказать, однако есть еще множество факторов, такие как время работы, сложность модели, еее адаптируемость, которые очень важны при выборе модели, и порой надо делать выбор в их пользу.\n",
    "  \n",
    "  \n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\* Несколько фрагментов кода в задании написаны на основе материалов [курса по глубинному обучению на ФКН НИУ ВШЭ](https://www.hse.ru/ba/ami/courses/205504078.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
