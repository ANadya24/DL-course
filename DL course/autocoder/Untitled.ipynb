{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "\n",
    "\"\"\"\n",
    "Все тензоры в задании имеют тип данных float32.\n",
    "\"\"\"\n",
    "\n",
    "class AE(nn.Module):\n",
    "    def __init__(self, d, D):\n",
    "        \"\"\"\n",
    "        Инициализирует веса модели.\n",
    "        Вход: d, int - размерность латентного пространства.\n",
    "        Вход: D, int - размерность пространства объектов.\n",
    "        \"\"\"\n",
    "        super(type(self), self).__init__()\n",
    "        self.d = d\n",
    "        self.D = D\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(self.D, 200),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(200, self.d)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.d, 200),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(200, self.D),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        Генерирует код по объектам.\n",
    "        Вход: x, Tensor - матрица размера n x D.\n",
    "        Возвращаемое значение: Tensor - матрица размера n x d.\n",
    "        \"\"\"\n",
    "        # ваш код здесь\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def decode(self, z):\n",
    "        \"\"\"\n",
    "        По матрице латентных представлений z возвращает матрицу объектов x.\n",
    "        Вход: z, Tensor - матрица n x d латентных представлений.\n",
    "        Возвращаемое значение: Tensor, матрица объектов n x D.\n",
    "        \"\"\"\n",
    "        # ваш код здесь\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def batch_loss(self, batch):\n",
    "        \"\"\"\n",
    "        Вычисляет функцию потерь по батчу - усреднение функции потерь\n",
    "        по объектам батча.\n",
    "        Функция потерь по объекту- сумма L2-ошибки восстановления по батчу и\n",
    "        L2 регуляризации скрытых представлений с весом 1.\n",
    "        Возвращаемое значение должно быть дифференцируемо по параметрам модели (!).\n",
    "        Вход: batch, Tensor - матрица объектов размера n x D.\n",
    "        Возвращаемое значение: Tensor, скаляр - функция потерь по батчу.\n",
    "        \"\"\"\n",
    "        # ваш код здесь\n",
    "        batch_size = batch.shape[0]\n",
    "        compute_loss = nn.MSELoss()\n",
    "        z = self.encode(batch)\n",
    "        l2_reg = torch.sum(z**2)\n",
    "        output = self.decode(z)\n",
    "        loss = torch.sum((output - batch)**2, dim=-1).mean(dim=0) + l2_reg\n",
    "        return loss\n",
    "\n",
    "    def generate_samples(self, num_samples):\n",
    "        \"\"\"\n",
    "        Генерирует сэмплы объектов x. Использует стандартное нормальное\n",
    "        распределение в пространстве представлений.\n",
    "        Вход: num_samples, int - число сэмплов, которые надо сгененрировать.\n",
    "        Возвращаемое значение: Tensor, матрица размера num_samples x D.\n",
    "        \"\"\"\n",
    "        # ваш код здесь\n",
    "        mu = torch.zeros(num_samples, self.D)\n",
    "        sigma = torch.ones(num_samples, self.D)\n",
    "        x = torch.normal(mu, sigma)\n",
    "        return self.decode(self.encode(x))\n",
    "            \n",
    "\n",
    "\n",
    "def log_mean_exp(data):\n",
    "    \"\"\"\n",
    "    Возвращает логарифм среднего по последнему измерению от экспоненты данной матрицы.\n",
    "    Подсказка: не забывайте про вычислительную стабильность!\n",
    "    Вход: mtx, Tensor - тензор размера n_1 x n_2 x ... x n_K.\n",
    "    Возвращаемое значение: Tensor, тензор размера n_1 x n_2 x ,,, x n_{K - 1}.\n",
    "    \"\"\"\n",
    "    # ваш код здесь\n",
    "    n = data.shape[-1]\n",
    "    output = torch.logsumexp(data, dim=-1) - torch.log(torch.tensor(n).float())\n",
    "    return output\n",
    "\n",
    "\n",
    "def log_likelihood(x_true, x_distr):\n",
    "    \"\"\"\n",
    "    Вычисляет логарфм правдоподобия объектов x_true для индуцированного\n",
    "    моделью покомпонентного распределения Бернулли.\n",
    "    Каждому объекту из x_true соответствуют K сэмплированных распределений\n",
    "    на x из x_distr.\n",
    "    Требуется вычислить оценку логарифма правдоподобия для каждого объекта.\n",
    "    Подсказка: не забывайте про вычислительную стабильность!\n",
    "    Подсказка: делить логарифм правдоподобия на число компонент объекта не надо.\n",
    "\n",
    "    Вход: x_true, Tensor - матрица объектов размера n x D.\n",
    "    Вход: x_distr, Tensor - тензор параметров распределений Бернулли\n",
    "    размера n x K x D.\n",
    "    Выход: Tensor, матрица размера n x K - оценки логарифма правдоподобия\n",
    "    каждого сэмпла.\n",
    "    \"\"\"\n",
    "    # ваш код здесь\n",
    "    log_l = x_true[:,None,:] * torch.log(x_distr) + (\n",
    "        1 - x_true[:, None, :]) * torch.log(1 - x_distr)\n",
    "    return log_l.sum(dim=-1)\n",
    "\n",
    "\n",
    "def kl(q_distr, p_distr):\n",
    "    \"\"\"\n",
    "    Вычисляется KL-дивергенция KL(q || p) между n парами гауссиан.\n",
    "    Вход: q_distr, tuple(Tensor, Tensor). Каждый Tensor - матрица размера n x d.\n",
    "    Первый - mu, второй - sigma.\n",
    "    Вход: p_distr, tuple(Tensor, Tensor). Аналогично.\n",
    "    Возвращаемое значение: Tensor, вектор размерности n, каждое значение которого - \n",
    "    - KL-дивергенция между соответствующей парой распределений.\n",
    "    \"\"\"\n",
    "    p_mu, p_sigma = p_distr\n",
    "    q_mu, q_sigma = q_distr\n",
    "    # ваш код здесь\n",
    "    n, d = p_mu.shape\n",
    "    kulbak = torch.log(p_sigma/q_sigma) - 0.5 + (q_sigma**2 + (q_mu - p_mu)**2) / (2 * p_sigma**2)\n",
    "    kulbak = torch.sum(kulbak, dim=1)\n",
    "    return kulbak\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, d, D):\n",
    "        \"\"\"\n",
    "        Инициализирует веса модели.\n",
    "        Вход: d, int - размерность латентного пространства.\n",
    "        Вход: D, int - размерность пространства объектов.\n",
    "        \"\"\"\n",
    "        super(type(self), self).__init__()\n",
    "        self.d = d\n",
    "        self.D = D\n",
    "        self.proposal_network = nn.Sequential(\n",
    "            nn.Linear(self.D, 200),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        self.proposal_mu_head = nn.Linear(200, self.d)\n",
    "        self.proposal_sigma_head = nn.Linear(200, self.d)\n",
    "        self.generative_network = nn.Sequential(\n",
    "            nn.Linear(self.d, 200),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(200, self.D),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def proposal_distr(self, x):\n",
    "        \"\"\"\n",
    "        Генерирует предложное распределение на z.\n",
    "        Подсказка: областью значений sigma должны быть положительные числа.\n",
    "        Для этого при генерации sigma следует использовать softplus (!) в качестве\n",
    "        последнего преобразования.\n",
    "        Вход: x, Tensor - матрица размера n x D.\n",
    "        Возвращаемое значение: tuple(Tensor, Tensor),\n",
    "        Каждый Tensor - матрица размера n x d.\n",
    "        Первый - mu, второй - sigma.\n",
    "        \"\"\"\n",
    "        # ваш код здесь\n",
    "        temp = self.proposal_network(x)\n",
    "        mu = self.proposal_mu_head(temp)\n",
    "        sigma = self.proposal_sigma_head(temp)\n",
    "        sigma = nn.Softplus()(sigma)\n",
    "        return (mu, sigma)\n",
    "\n",
    "    def prior_distr(self, n):\n",
    "        \"\"\"\n",
    "        Генерирует априорное распределение на z.\n",
    "        Вход: n, int - число распределений.\n",
    "        Возвращаемое значение: tuple(Tensor, Tensor),\n",
    "        Каждый Tensor - матрица размера n x d.\n",
    "        Первый - mu, второй - sigma.\n",
    "        \"\"\"\n",
    "        # ваш код здесь\n",
    "        mu = torch.zeros(n, self.d)\n",
    "        sigma = torch.ones(n, self.d)\n",
    "        return mu, sigma\n",
    "\n",
    "    def sample_latent(self, distr, K=1):\n",
    "        \"\"\"\n",
    "        Генерирует сэмплы из гауссовского распределения на z.\n",
    "        Сэмплы должны быть дифференцируемы по параметрам распределения!\n",
    "        Вход: distr, tuple(Tensor, Tensor). Каждое Tensor - матрица размера n x d.\n",
    "        Первое - mu, второе - sigma.\n",
    "        Вход: K, int - число сэмплов для каждого объекта.\n",
    "        Возвращаемое значение: Tensor, матрица размера n x K x d.\n",
    "        \"\"\"\n",
    "        # ваш код здесь\n",
    "        mu, sigma = distr\n",
    "        n, d = sigma.size()\n",
    "        samples = torch.randn((n, K, d))\n",
    "        samples = mu[:, None, :] + sigma[:, None, :] * samples\n",
    "        return samples\n",
    "\n",
    "    def generative_distr(self, z):\n",
    "        \"\"\"\n",
    "        По матрице латентных представлений z возвращает матрицу параметров\n",
    "        распределения Бернулли для сэмплирования объектов x.\n",
    "        Вход: z, Tensor - тензор n x K x d латентных представлений.\n",
    "        Возвращаемое значение: Tensor, тензор параметров распределения\n",
    "        Бернулли размера n x K x D.\n",
    "        \"\"\"\n",
    "        # ваш код здесь\n",
    "        x = self.generative_network(z)\n",
    "        eps = 1e-4\n",
    "        return torch.clamp(x, eps, 1-eps)\n",
    "    \n",
    "    def batch_loss(self, batch):\n",
    "        \"\"\"\n",
    "        Вычисляет вариационную нижнюю оценку логарифма правдоподобия по батчу.\n",
    "        Вариационная нижняя оценка должна быть дифференцируема по параметрам модели (!),\n",
    "        т. е. надо использовать репараметризацию.\n",
    "        Требуется вернуть усреднение вариационных нижних оценок объектов батча.\n",
    "        Вход: batch, FloatTensor - матрица объектов размера n x D.\n",
    "        Возвращаемое значение: Tensor, скаляр - вариационная нижняя оценка логарифма\n",
    "        правдоподобия по батчу.\n",
    "        \"\"\"\n",
    "        # ваш код здесь\n",
    "        batch_size = batch.shape[0]\n",
    "        q = self.proposal_distr(batch)\n",
    "        z = self.sample_latent(q)\n",
    "        p = self.generative_distr(z)\n",
    "        log_p = log_likelihood(batch, p)\n",
    "        rec_loss = log_mean_exp(log_p)\n",
    "        kulbak = kl(q, self.prior_distr(batch_size))\n",
    "        return (rec_loss - kulbak).mean()\n",
    "\n",
    "    def generate_samples(self, num_samples):\n",
    "        \"\"\"\n",
    "        Генерирует сэмплы из индуцируемого моделью распределения на объекты x.\n",
    "        Вход: num_samples, int - число сэмплов, которые надо сгененрировать.\n",
    "        Возвращаемое значение: Tensor, матрица размера num_samples x D.\n",
    "        \"\"\"\n",
    "        # ваш код здесь\n",
    "        z = self.prior_distr(num_samples)\n",
    "        x = self.generative_distr(z)\n",
    "        return x\n",
    "\n",
    "\n",
    "def gaussian_log_pdf(distr, samples):\n",
    "    \"\"\"\n",
    "    Функция вычисляет логарифм плотности вероятности в точке относительно соответствующего\n",
    "    нормального распределения, заданного покомпонентно своими средним и среднеквадратичным отклонением.\n",
    "    Вход: distr, tuple(Tensor, Tensor). Каждый Tensor - матрица размера n x d.\n",
    "    Первый - mu, второй - sigma.\n",
    "    Вход: samples, Tensor - тензор размера n x K x d сэмплов в скрытом пространстве.\n",
    "    Возвращаемое значение: Tensor, матрица размера n x K, каждый элемент которой - логарифм\n",
    "    плотности вероятности точки относительно соответствующего распределения.\n",
    "    \"\"\"\n",
    "    mu, sigma = distr\n",
    "    # ваш код здесь\n",
    "    c = - 0.5 * torch.log(torch.tensor(2 * math.pi))\n",
    "    tmp = -1 * (torch.log(sigma[:, None, :]) + (samples - mu[:, None, :])**2/(2 * sigma[:, None, :]**2))\n",
    "    return (c + tmp).sum(dim=-1)\n",
    "    \n",
    "\n",
    "def compute_log_likelihood_monte_carlo(batch, model, K):\n",
    "    \"\"\"\n",
    "    Функция, оценку логарифма правдоподобия вероятностной модели по батчу методом Монте-Карло.\n",
    "    Оценка логарифма правдоподобия модели должна быть усреднена по всем объектам батча.\n",
    "    Подсказка: не забудьте привести возращаемый ответ к типу float, иначе при вычислении\n",
    "    суммы таких оценок будет строится вычислительный граф на них, что быстро приведет к заполнению\n",
    "    всей доступной памяти.\n",
    "    Вход: batch, FloatTensor - матрица размера n x D\n",
    "    Вход: model, Module - объект, имеющий методы prior_distr, sample_latent и generative_distr,\n",
    "    описанные в VAE.\n",
    "    Вход: K, int - количество сэмплов.\n",
    "    Возвращаемое значение: float - оценка логарифма правдоподобия.\n",
    "    \"\"\"\n",
    "    # ваш код здесь\n",
    "    batch_size = batch.shape[0]\n",
    "    prior_z = model.prior_distr(batch_size)\n",
    "    samples = model.sample_latent(prior_z, K)\n",
    "    p = model.generative_distr(samples)\n",
    "    log_p = log_likelihood(batch, p)\n",
    "    output = log_mean_exp(log_p).sum()\n",
    "    return output.data.numpy() \n",
    "\n",
    "\n",
    "def compute_log_likelihood_iwae(batch, model, K):\n",
    "    \"\"\"\n",
    "    Функция, оценку IWAE логарифма правдоподобия вероятностной модели по батчу.\n",
    "    Оценка логарифма правдоподобия модели должна быть усреднена по всем объектам батча.\n",
    "    Подсказка: не забудьте привести возращаемый ответ к типу float, иначе при вычислении\n",
    "    суммы таких оценок будет строится вычислительный граф на них, что быстро приведет к заполнению\n",
    "    всей доступной памяти.\n",
    "    Вход: batch, FloatTensor - матрица размера n x D\n",
    "    Вход: model, Module - объект, имеющий методы prior_distr, proposal_distr, sample_latent и generative_distr,\n",
    "    описанные в VAE.\n",
    "    Вход: K, int - количество сэмплов.\n",
    "    Возвращаемое значение: float - оценка логарифма правдоподобия.\n",
    "    \"\"\"\n",
    "    # ваш код здесь\n",
    "    batch_size = batch.shape[0]\n",
    "    prior_z = model.prior_distr(batch_size)\n",
    "    q = model.proposal_distr(batch)\n",
    "    samples = model.sample_latent(q, K)\n",
    "    p = model.generative_distr(samples)\n",
    "    log_p = log_likelihood(batch, p)\n",
    "    log_prior = gaussian_log_pdf(prior_z, samples)\n",
    "    log_q = gaussian_log_pdf(q, samples)\n",
    "    log_total = log_p + log_prior - log_q\n",
    "    output = log_mean_exp(log_total).sum()\n",
    "    return output.data.numpy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_loss(batch):\n",
    "        \"\"\"\n",
    "        Вычисляет вариационную нижнюю оценку логарифма правдоподобия по батчу.\n",
    "        Вариационная нижняя оценка должна быть дифференцируема по параметрам модели (!),\n",
    "        т. е. надо использовать репараметризацию.\n",
    "        Требуется вернуть усреднение вариационных нижних оценок объектов батча.\n",
    "        Вход: batch, FloatTensor - матрица объектов размера n x D.\n",
    "        Возвращаемое значение: Tensor, скаляр - вариационная нижняя оценка логарифма\n",
    "        правдоподобия по батчу.\n",
    "        \"\"\"\n",
    "        # ваш код здесь\n",
    "        batch_size = batch.shape[0]\n",
    "        mu, sigma = self.proposal_distr(batch)\n",
    "        z = self.sample_latent(mu, sigma)\n",
    "        X = self.generative_distr(z)\n",
    "        rec_loss = torch.mean(X) / batch_size\n",
    "        kl = torch.mean(kl((mu, sigma), self.prior_distr(batch_size)))\n",
    "        return rec_loss + kl\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-d595752af008>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-49-102c7e5383e9>\u001b[0m in \u001b[0;36mbatch_loss\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# ваш код здесь\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproposal_distr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_latent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerative_distr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "batch = torch.randn(10, 30)\n",
    "d = batch_loss(batch)\n",
    "print(d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_log_pdf(distr, samples):\n",
    "    \"\"\"\n",
    "    Функция вычисляет логарифм плотности вероятности в точке относительно соответствующего\n",
    "    нормального распределения, заданного покомпонентно своими средним и среднеквадратичным отклонением.\n",
    "    Вход: distr, tuple(Tensor, Tensor). Каждый Tensor - матрица размера n x d.\n",
    "    Первый - mu, второй - sigma.\n",
    "    Вход: samples, Tensor - тензор размера n x K x d сэмплов в скрытом пространстве.\n",
    "    Возвращаемое значение: Tensor, матрица размера n x K, каждый элемент которой - логарифм\n",
    "    плотности вероятности точки относительно соответствующего распределения.\n",
    "    \"\"\"\n",
    "    mu, sigma = distr\n",
    "    # ваш код здесь\n",
    "    c = -0.5 * torch.log(torch.tensor(2 * math.pi))\n",
    "    tmp = -1 * (torch.log(sigma[:, None, :]) + (samples - mu[:, None, :])**2/(2 * sigma[:, None, :]**2))\n",
    "    return (c + tmp).mean(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-7a2d183ad060>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgaussian_log_pdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m==\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "mu = torch.zeros((10, 20))\n",
    "sigma = torch.ones((10, 20))\n",
    "samples = torch.randn((10, 5, 20))\n",
    "o = gaussian_log_pdf((mu, sigma), samples)\n",
    "print(o.shape, o== b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_mean_exp(data):\n",
    "    \"\"\"\n",
    "    Возвращает логарифм среднего по последнему измерению от экспоненты данной матрицы.\n",
    "    Подсказка: не забывайте про вычислительную стабильность!\n",
    "    Вход: mtx, Tensor - тензор размера n_1 x n_2 x ... x n_K.\n",
    "    Возвращаемое значение: Tensor, тензор размера n_1 x n_2 x ,,, x n_{K - 1}.\n",
    "    \"\"\"\n",
    "    # ваш код здесь\n",
    "    n = data.shape[-1]\n",
    "    output = torch.logsumexp(data, dim=-1) - torch.log(torch.tensor(n).float())\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 20])\n"
     ]
    }
   ],
   "source": [
    "data = torch.randn((1,10,20,30))\n",
    "a = log_mean_exp(data)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_likelihood(x_true, x_distr):\n",
    "    \"\"\"\n",
    "    Вычисляет логарфм правдоподобия объектов x_true для индуцированного\n",
    "    моделью покомпонентного распределения Бернулли.\n",
    "    Каждому объекту из x_true соответствуют K сэмплированных распределений\n",
    "    на x из x_distr.\n",
    "    Требуется вычислить оценку логарифма правдоподобия для каждого объекта.\n",
    "    Подсказка: не забывайте про вычислительную стабильность!\n",
    "    Подсказка: делить логарифм правдоподобия на число компонент объекта не надо.\n",
    "\n",
    "    Вход: x_true, Tensor - матрица объектов размера n x D.\n",
    "    Вход: x_distr, Tensor - тензор параметров распределений Бернулли\n",
    "    размера n x K x D.\n",
    "    Выход: Tensor, матрица размера n x K - оценки логарифма правдоподобия\n",
    "    каждого сэмпла.\n",
    "    \"\"\"\n",
    "    # ваш код здесь\n",
    "    log_l = x_true[:,None,:] * torch.log(x_distr) + (\n",
    "        1 - x_true[:, None, :]) * torch.log(1 - x_distr)\n",
    "    return log_l.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5])\n"
     ]
    }
   ],
   "source": [
    "x_true = torch.randn((10,20))\n",
    "x_distr = torch.randn((10,5,20))\n",
    "b = log_likelihood(x_true, x_distr)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_latent(distr, K=1):\n",
    "        \"\"\"\n",
    "        Генерирует сэмплы из гауссовского распределения на z.\n",
    "        Сэмплы должны быть дифференцируемы по параметрам распределения!\n",
    "        Вход: distr, tuple(Tensor, Tensor). Каждое Tensor - матрица размера n x d.\n",
    "        Первое - mu, второе - sigma.\n",
    "        Вход: K, int - число сэмплов для каждого объекта.\n",
    "        Возвращаемое значение: Tensor, матрица размера n x K x d.\n",
    "        \"\"\"\n",
    "        # ваш код здесь\n",
    "        mu, sigma = distr\n",
    "        n, d = sigma.size()\n",
    "#         samples = torch.randn((n, K, d))\n",
    "#         samples = mu[:, None, :] + sigma[:, None, :] * samples\n",
    "#         return samples\n",
    "        std = torch.log(torch.exp(sigma[:, None, :] - 1))\n",
    "        eps = torch.randn((n, K, d))\n",
    "        return eps.mul(std).add_(mu[:, None, :])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
